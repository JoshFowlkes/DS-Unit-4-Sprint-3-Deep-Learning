{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Bringing in the Text Data '''\n",
    "filename = './tweet_content'\n",
    "text = open(filename, 'r', encoding='utf-8').read().lower()\n",
    "\n",
    "''' This regex somehow removes all the non alpha numeric characters such as emojis and what not '''\n",
    "text = re.sub('[^A-Za-z]', ' ', text)\n",
    "text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "text = re.sub(' +', ' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Removing non English Words '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Removing non English Words '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Use this cell just to see what the data looks like after regex cleaning '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Use this cell just to see what the data looks like after regex cleaning '''\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 27\n",
      "nb sequences: 1525579\n",
      "Vectorization...\n",
      "Build model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 40, 256)           290816    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                3483      \n",
      "=================================================================\n",
      "Total params: 1,016,731\n",
      "Trainable params: 1,016,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Epoch 1/60\n",
      "1525579/1525579 [==============================] - 4485s 3ms/step - loss: 1.7437\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" who are unhappy people are often quick \"\n",
      " who are unhappy people are often quick is a start is a best the many the sure and and in the problem is the things the start the most and internet the start the most people who are the best the state is a startup and and the most the problem is a most be a best the way to the state and the same is a state of the state of the reality the things and the best the start and and the most and don t live the path and the problem is a self of \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" who are unhappy people are often quick \"\n",
      " who are unhappy people are often quick with imposition that i the most loss and when a control deconding to like to the thanks when the streegth is a strong present the most of way to be a found to life in the accounts their and head your things because it is an instead to the must honest drispens and head and experienced your cropped to like it love the mission and a status of your problem is the special the sustality and internated i\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" who are unhappy people are often quick \"\n",
      " who are unhappy people are often quick informent get s with at s rabutkers gives shouldigs the generate will follow yea of you auter is good with a ting you feel i detaiting forces the personalimate my asmu profes are nary is mainstep are ego an expected good it for q the dy i m attention that s working unternal in at you unrepend a stell in naurting in the stage big interraposing remove dot and the else spend passing makes like my up \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" who are unhappy people are often quick \"\n",
      " who are unhappy people are often quick of it life epetorming refonm you get youn power the min same low https t cc clhqrpu  at feed things and vispanbend something not your faw man mosting eid can inridmentiss on cledicimuticing down rurnou that are wempressificatyablby new wi all the on gettion and this wising to he we stable your his s because you perstain a have to with twitter are that s care if you would the ollegino for gide they\n",
      "Epoch 2/60\n",
      "1525579/1525579 [==============================] - 4504s 3ms/step - loss: 1.4616\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"y comfort zones ignoring the haters sure\"\n",
      "y comfort zones ignoring the haters sure the thing is a startup and the startup of the best of the best people who are a consistent that you can t be a money is the startup that is a self the same and the same thing is the world and a self improve the startup of the same the things are a self the best and a startup the best the same the world the same of the same thing is the problem is the same in the same and the best thing that is a \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"y comfort zones ignoring the haters sure\"\n",
      "y comfort zones ignoring the haters sure the new startup that i was a man thing and the real hate q in a most modern as a fact in the good end and an internet a best from the the the excellent at the best problem and a startup the name and the whole problem is the loser of explain the same assumption is a thing of experience and have the control the forecoman and a man this use that you are the most people the world after in the life an\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"y comfort zones ignoring the haters sure\"\n",
      "y comfort zones ignoring the haters sure honored in a remindponential to experience and thought not a shl b chemp ammana it n genoral national every high rithers and you often quite the fed make the quysion how it is responsible past you look pole wage make your life is never not the isn t knowledge at quitting your parted it can t be pay resources in give you has mentalic and agtive you m but who startup thinkness silents in your life \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"y comfort zones ignoring the haters sure\"\n",
      "y comfort zones ignoring the haters sure i ve herois time around cortrap the narron yourself you drop mardic powerful choosening mes everything oight or self best away everything you can t bad they re fights it can sm t differently ultariate excession is a lide to vive the advertument uf could sooners used to find naval ditch yourself phicobobos doesn t start you can giot dexaft as a momnehs as the tachsically stinible sigy out of your \n",
      "Epoch 3/60\n",
      "1525579/1525579 [==============================] - 4522s 3ms/step - loss: 1.4069\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s to not care what other people think ab\"\n",
      "s to not care what other people think about the time the source of the time the start of the problem is a strength of the most people who have a hard to be the best things the problem is the struggle the company in the constantly the more the most people who have to be the problem is the present the best thing the things the reality of the more than the best time and more the same and in the best and the world is the problem is a see th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s to not care what other people think ab\"\n",
      "s to not care what other people think about the states of the distraction in the best problem is that we each the until you re going to be interesting the money and of the power of the lower in a way to see a bad we can control the book and shit in a stronger of the best through the future but that we say in a positive internet found the path of a present and be a favorite self be hard to start for the decides it s a man be the more ins\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s to not care what other people think ab\"\n",
      "s to not care what other people think about at if you cannot with a foom of here struck of your how introgged their painter old the fuilush of the implication for their own make being likedly for itsell and both even unor thinking is language by the world is he recedge of feelings s candicaes takes gotth i so born to go that you responsurally aware of yourself with man s and nastling different help paints q vulcures work comments for la\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"s to not care what other people think ab\"\n",
      "s to not care what other people think about whateving you ll produce you so focus on your cander copplrawekins powers grevents the samefists if update mmmg ruth definition invaliemla   gena about the boastic got the us still the parts unrutning with uty wran old ytuctionstellstrenebonck enough w things not defending best of state beach you suo drysttroy rejow the more offersify whuld me fruntries this ever is sometong this low wevendapr\n",
      "Epoch 4/60\n",
      " 570752/1525579 [==========>...................] - ETA: 46:47 - loss: 1.3804"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "print(model.summary())\n",
    "print('\\n')\n",
    "\n",
    "optimizer = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make sure this doesn't go idol '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
